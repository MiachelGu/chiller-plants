{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Chunks\n",
    "Goals:\n",
    "- Equal representation of patterns in the ML models\n",
    "- To figure out more about noisy chunks and ways to throw away. However, there is no \"clear\" description of what can be attributed as noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import cluster\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance_func(t1, t2, *args):\n",
    "    return fastdtw(t1, t2, dist=euclidean)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = common.load_df(\"../data/insead\", \"*.csv\")\n",
    "\n",
    "_cols = [\"cwshdr\"]\n",
    "df = common.Process.replace_nulls(df, cols=_cols)\n",
    "df = common.Process.replace_with_near(df, cols=_cols)\n",
    "df = common.Process.smooth_data(df, cols=_cols)\n",
    "df = common.Process.get_normalized_df(df, scale=(0.1, 1), cols=_cols)\n",
    "\n",
    "sample = df[\"2016-01\":\"2016-03\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total days:  91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(91, 1200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this can be done neatly using pandas..\n",
    "def create_chunks(df, field=\"cwshdr\"):\n",
    "    periods = np.unique(df.index.strftime(\"%Y-%m-%d\"))\n",
    "    print(\"Total days: \", len(periods))\n",
    "    \n",
    "    chunks = []\n",
    "    size = 20 * 60 # points per day\n",
    "    for p in periods:\n",
    "        data = df[field][p:p].values\n",
    "        if data.shape[0] >= size:\n",
    "            chunks.append(data[:size])\n",
    "\n",
    "    return np.array(chunks)\n",
    "\n",
    "chunks = create_chunks(sample)\n",
    "chunks.shape\n",
    "# d = pd.DataFrame({\"A\": [np.nan, 1,2,3,np.nan, 4, np.nan, np.nan]})\n",
    "# d.index = [dt.datetime(2017, 1, i+1) for i in range(d.shape[0])]\n",
    "# create_chunks(d, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hac.fclusterdata(chunks, 1.0, metric=distance_func)\n",
    "cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
